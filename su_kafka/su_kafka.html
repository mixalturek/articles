<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <title>Patálie s Apache Kafkou a /bin/su</title>

    <style media="all">
        body { background-color: silver; font-family: sans-serif; text-align: justify; line-height: 1.4em; }
        #page { margin: auto; max-width: 70%; background-color: white; padding: 2em; border: 1px solid black; }
        #perex { font-style: italic; }
        .img { text-align: center; overflow: auto; }
        pre { border: 1px solid gray; padding: 1em; background-color: #eeeeee; overflow: auto; }
        .highlight { font-weight: bold; color: orange; }
        div .side { border: 1px solid gray; margin: 1em 0 1em 1em; padding: 0 1em 0 1em; background-color: #eeeeee; float: right; max-width: 25%; }
    </style>
</head>

<body>
<div id="page">

<h1>Patálie s Apache Kafkou a /bin/su</h1>

<p id="perex">Když se jeden Linux admin a jeden backend vývojář rozhodnou vyzkoumat příčinu problému, který je pravidelně otravuje při restartech distribuované cloudové služby, můžou se postupně ponořit opravdu hluboko...</p>

<p>Tento příběh začal, jako většina podobných investigací, příchozími alerty o chybách na produkci. Pravidelně spouštěné integrační testy začaly po aktualizaci a plánovaném restartu jednoho z nodů <a href="http://kafka.apache.org/">Apache Kafka</a> clusteru hlásit duplikace v konzumovaných zprávách.</p>

<p>Nejsme banka, a tudíž v naší konfiguraci může taková situace občas (zřídka) nastat, nicméně všechno má své meze. Pokud není množství chyb v poměru k celkovému počtu událostí statisticky významné, není třeba nic řešit. Ale je velký rozdíl mezi jednotkami až stovkami duplicit denně nebo tisíci za vteřinu po dobu několika hodin. Byl to zkrátka takový ošklivý... nepěkná věc, která se už neututlá a raději se s ní jde rovnou ven, protože by si jí stějně někdo brzy všimnul.</p>

<p>Začátek všech alertů v čase celkem jasně korespondoval s aktualizací prvního Kafka brokeru a následným restartem. Naše dosavadní zkušenosti s rolling restarty Kafky říkají, že se jedná spíše o křehkou operaci, při které se dost často něco po... pokazí a pokaždé nějak úplně jinak. Kreativita distribuovaných systémů dokáže vždy překvapit. Pokud máte s graceful restarty Kafky jiné, lepší zkušenosti, čtěte dál, právě o tom je tento článek. My už je snad budeme mít v budoucnu také - <em>zkrátka chyba byla u nás a Kafka za nic nemohla</em>.</p>

<p>Během analýzy si osvěžíme principy operačních systémů, setkáme se s tucty známých i méně známých termínů, objevíme naprosto nečekané rozdíly v RedHat a Debian based distribucích a také se mimo jiné zanoříme do zdrojových kódů jistého unixového nástroje, který lidstvo denně používá už desítky let. Text se může zdát místy trochu složitější, ale nenechte se odradit, čtení až do konce bude určitě stát za to.</p>

<p>Aby čtenář pochopil, jak vysoko se na aplikačním stacku běžně pohybujeme a jak hluboko do operačního systému jsme se přes všechny vrstvy se svými analýzami následně dostali, je třeba nejdříve nastínit, jak vypadá celková architektura.</p>


<div class="side">
    <h3>Apache Kafka</h3>
    <p><a href="http://kafka.apache.org/">Apache Kafka</a> je platforma pro streamování dat. Sestává z distribuované serverové služby a dvojice klientských knihoven umožňujících produkci a konzumaci událostí. Veškeré záznamy drží na pevných discích typicky po dobu několika dní a zároveň je replikuje do více instancí pro případ selhání některé z nich. Horizontálně škáluje.</p>

    <p>Ačkoli veškerá přenášená data ukládá na disk, rozhodně není pomalá. Sekvenční čtení/zápis většího množství bufferovaných dat z/na disk je rychlostně srovnatelný s přístupem do paměti. Kafka dále využívá <a href="http://kafka.apache.org/documentation/#maximizingefficiency">různé triky</a> jako například přímé kopírování z disku na síťovou kartu v kernel space. Tím se obchází zbytečné kopírování bufferů do user space a zpět. Autor článku už několikrát ve čtyř-nodovém clusteru pozoroval datové přenosy stovek MB/s, resp. vyšších stovek tisíc zpráv/s. Limitním faktorem bývá spíše rychlost síťových propojů než disků nebo aplikačního kódu Kafky.</p>

    <p>V nejjednodušší formě se Kafka používá k oddělení aplikací produkujících události od aplikací, které je zpracovávají. Produceři a konzumeři jsou naprosto nezávislé systémy a mají plně oddělený životní cyklus, spojuje je pouze formát přenášených zpráv.</p>

    <p>Kafka bývá často srovnávána s messaging systémy typu <a href="https://www.rabbitmq.com/">RabbitMQ</a>. Narozdíl od nich garantuje pořadí doručení událostí - v jádru své architektury obsahuje datovou strukturu <a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">log</a>. Díky tomuto napohled malému rozdílu může být jednou z jejích dalších aplikací i <a href="https://www.confluent.io/blog/turning-the-database-inside-out-with-apache-samza/">primární databáze systému</a>, kdy tradiční databáze a úložiště (relační SQL databáze, <a href="http://hadoop.apache.org/">Hadoop</a>, <a href="https://www.elastic.co/">Elasticsearch</a>, <a href="http://cassandra.apache.org/">Cassandra</a>, <a href="https://redis.io/">Redis</a>, atd.) mají pouze formu <em>materialized view</em> a dají se kdykoli přepočítat.</p>
</div>


<h2>Z výšky jednoho kilometru</h2>

<p>Námi vyvinutý systém pro <em>big data</em> analýzy (v odpoledních špičkách řádově 100k událostí/s, 100 MB/s) používá na příjmové straně dvě vrstvy oddělených Kafka clusterů. První skupina backendů toho času běží ve čtyřech nezávislých datacentrech rozmístěných různě po světě. Především aby byly co nejblíže aplikacím u uživatelů produkujících události a zároveň dokázaly překlenout nejrůznější síťové nedostupnosti a nespolehlivosti internetu.</p>

<p>Do Kafka clusterů proudí vstupní data výhradně skrz aplikaci <em>Receiver</em>, jedná se v podstatě o <em>REST-like proxy</em> obsahující Kafka producer klienta a nepříliš složitou logiku. Ve chvíli, kdy Kafka potvrdí uložení přijaté události, máme v případě možných problémů teoreticky až několik dnů na opravu, znovuvyčtení a zpracování perzistovaných dat. Taková situace by rozhodně nebyla příjemná a určitě bychom se zapotili, nicméně by nemělo dojít k žádné ztrátě dat. Receivery a Kafky jsou tedy základem stability celého systému a musí vždy spolehlivě běžet.</p>

<p>Události z Kafky vyčítají <em>Input Filtry</em>, jejichž úkolem je konverze starších formátů na nový, čištění dat, opravy chyb způsobených bugy v historických verzích klientů, obohacení událostí o GeoIP lokace a podobně. Data se zapisují zpět do té samé DC Kafky, ale do topiků s jinými jmény.</p>

<p>Vyčištěné události se z DC Kafka clusterů mirrorují pomocí standartního Mirror maker toolu do tzv. master Kafky, odkud je dále vyčítají finální konzumeři do <a href="http://hadoop.apache.org/">Hadoopu/HDFS</a>, vybrané topiky tečou do <a href="https://www.elastic.co/">Elasticsearche</a>, malá frakce se sampluje na stage a dev prostředí, popř. nad streamem běží nejrůznější realtime analýzy. Kafka otevírá spoustu možností.</p>

<p class="img"><img src="img/kafka_pipeline.png" /></p>

<p>Každá z aplikací běží replikovaně v několika instancích. Pro rychlou představu: Receiverů a Input Filtrů je 11, Kafka brokerů celkem 14, v rámci každého Kafka clusteru musí samozřejmě běžet i cluster <a href="https://zookeeper.apache.org/">ZooKeeperů</a>. Data z každého datacentra kopíruje 5 instancí Mirror makerů. Všichni konzumer klienti se dále replikují nejen na úrovni instancí a strojů, ale, aby se zvýšil paralelismus, i pomocí konfigurovatelného počtu aplikačních konzumer vláken. Objem zpracovávaných dat s připojováním a rozšiřováním jednotlivých aplikací v čase neustále narůstá. Základním požadavkem je možnost snadného škálování na všech úrovních, jinak by systém nebyl už dávno udržitelný.</p>

<p>A v této změti distribuovaných aplikací, jejich jednotlivých instancí, strojů, aplikačních vláken, grafů a logů hledejte příčinu nějaké zřídka se vyskytující chyby...</p>


<h2>Duplicity, duplicity, duplicity</h2>

<p>Integrační testy, o nichž byla v úvodu řeč, simulují chování reálných klientů na obou stranách Kafka pipeline. Generují a zapisují události do Receiverů v jednotlivých datacentrech, následně je vyčítají z master Kafky a porovnávají přijatá data s očekávanými transformacemi. Každá z testovacích zpráv obsahuje unikátní <code>UUID.randomUUID()</code> pro zpárování vstupu a výstupu, bez toho by nemohly fungovat. Integrační testy si ale také ukládají množinu již přijatých UUID. Pokud se některé z nich na výstupu systému objeví více než jednou, je to jasná známka duplikace.</p>

<p>Jelikož počátek duplikací časově korespondoval s restartem Kafka brokeru ve franskfurtském datacentru, dalo se předpokládat, že je na vině některý ze systémů, který přímo komunikuje s Kafkou, tj. Receiver (producer), Filtry (konsumer, producer), ostatní Kafka brokeři ve frankfurtu (repliky) nebo Mirror maker (konzumer).</p>

<p>Jako viník byl nakonec podle grafů odhalen Mirror maker, kterému se v daném čase skokově zvýšil počet zkonzumovaných zpráv. V globálním pohledu to není tak zřejmé, ale při zaostření pouze na Frankfurkt bylo vše ihned jasné. Po restartu se datový přenos skokově snížil zpět na původní hodnotu a integrační testy znovu samým štěstím zezelenaly.</p>

<p class="img"><img src="img/duplicates_mirrors_all.png" /></p>
<p class="img"><img src="img/duplicates_mirrors_fra.png" /></p>

<p>Poznámka na okraj pro úplnost článku: Chybu v Mirror makeru a potažmo v libovolném Kafka konzumeru jsme nedokázali vystopovat. Na základě logů jsme měli několik teorií, ale všechny jsme bohužel dokázali celkem snadno vyvrátit. Bug report do Kafky nemělo cenu vytvářet, protože jsme neměli žádné vodítko, kterým směrem obrátit pozornost Kafka vývojářů. Ti jsou mimochodem velice aktivní a odpovídají prakticky okamžitě, už jsme pár chyb v minulosti hlásili.</p>


<h2>Zabíjíme Kafku</h2>

<p>Proč jsou restarty Kafky vždycky tak křehké jako napůl vysušený hrad z písku? A proč to nikdo jiný než my neřeší? Tohle přece musí vadit každému. Žádný ticket, žádný článek, žádný blog.</p>

<p>Chvíle ticha...</p>

<p>A co když je problém jenom u nás na serverech? Když spouštím Kafku u sebe při vývoji, je typicky všechno v pořádku. Ale je pravda, že u mě neteče třicet tisíc zpráv za vteřinu, může být rozdíl v tom? Pojďme to už jednou provždy vyřešit!</p>

<p>Pozorované symptomy</p>

<ul>
<li>Kafka po restartu vždy přepočítává spousty koruptovaných souborů s indexy. Tato operace běžně trvá i <strong>několik hodin</strong> a opravdu bolí. Jeden rolling restart celého clusteru trvá běžně celý den. Update na novou verzi potřebuje v obecném případě dva restarty a clusterů máme několik. Zkrátka obrovské au.</li>
<li>Pokud se Kafka po zastavení hned spustí, ZooKeeper ji odmítne, protože si myslí, že stále běží. Broker ID  je stále registrované a je třeba chvíli počkat na timeout.</li>
<li>Producerům se v okamžiku restartu nedaří zapisovat do partition, jejichž leader je při rebalancingu migrován na ostatní brokery z ISR (In Sync Replicas). Callback v klientu obdrží výjimku místo úspěšného potvrzení, nakonfigurovaný jeden retry nestačí.</li>
<li>Na producerech používáme <code>acks = 1</code>, tj. při odesílání dat stačí potvrzení leadera. Ve chvíli, kdy leader přestane být dostupný, se tudíž ztrácí zprávy, které ještě nebyly odeslány replikám. Řádově se jedná o desítky až stovky zpráv.</li>
<li>Konzumeři se občas škaredě zblázní. A když říkám škaredě, myslím tím opravdu <strong>škaredě</strong>. Výše jsou popsané duplikace, ale už jsme u několika mála partition a konzumer grup viděli i ztrátu offsetů, což by nemělo nikdy nastat. Podle logů byla pravděpodobně způsobená chvilkovou nedostupností dat. Nevalidní offset při požadavku na data spolu s nevhodnou konfigurací <code>auto.offset.reset = earliest</code> způsobil skok na začátek logu a následnou bolestivou rekonzumaci všech dat, tj. posledních několik dnů. Od jisté doby už nepoužíváme earlieast, ale latest s všudypřítomným komentářem, že je lepší přeskočit a ztratit pár minut, než zbytečně všechno vyčítat a znovuzpracovávat. Samozřejmě záleží na službě a očekáváních.</li>
</ul>

<p class="img"><img src="img/mirror_lag.png" /></p>

<p>Vypadá to, jako bychom Kafku neukončovali korektně. V hlavě se nám objevuje slovo <code>SIGKILL</code>, ale kdo by ho posílal?</p>


<h2>Ukončujeme Kafku</h2>

<p>Nyní jsou dvě možné varianty, buď máme Kafku špatně nakonfigurovanou nebo ji ukončujeme špatně. <a href="http://kafka.apache.org/documentation/#basic_ops_restarting">Dokumentace o restartech</a> hovoří jasně.</p>

<ul>
<li>Vypínaný server synchronizuje všechny logy na disk, aby zamezil potřebě jakýchkoli oprav. Případné opravy trvají dlouho a pokud nejsou nutné, úmyslné restarty budou rychlejší.</li>
<li>Vypínaný server namigruje všechny partition, u kterých je veden jako leader, na ostatní repliky. Toto minimalizuje čas, kdy je partition nedostupná na několik milisekund.</li>
</ul>

<p>Synchronizace souborů na disk by měla být aktivní vždy nezávisle na konfiguraci, migrace leaderů, pouze pokud je v konfiguraci uvedeno <code>controlled.shutdown.enable = true</code>. V konfiguračním souboru tuto volbu máme uvedenou, takže problém by měl být někde jinde.</p>

<p>Jak správně ukončit Kafku? Skript <code>kafka-server-stop.sh</code> používá signál <code>SIGTERM</code>.</p>

<pre>
#!/bin/sh
# ...
PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '{print $1}')

if [ -z "$PIDS" ]; then
  echo "No kafka server to stop"
  exit 1
else.
  kill -s TERM $PIDS
fi
</pre>

<p>Direktivu <code><a href="http://upstart.ubuntu.com/cookbook/#kill-signal">kill signal</a></code> v Upstart konfiguraci explicitně uvedenou nemáme, nicméně výchozí hodnotou je předpokládaný <code>SIGTERM</code>. Pokud by se proces ale nestihl ukončit, Upstart by ho po <a href="http://upstart.ubuntu.com/cookbook/#daemon-behaviour">kill timeoutu (ve výchozím stavu 5 sekund) zabil signálem <code>SIGKILL</code></a>. Ha! Pět sekund je opravdu hodně málo a toto by mohla být příčina všech našich problémů. Konfigurace však explicitně uvádí <code>kill timeout 300</code>, což není žádných 5 sekund ale 5 minut. Autor zde byl opravdu štědrý a chtěl mít jistotu, že Kafka bude mít dost času. Na všech úrovních bychom tedy měli mít všechno nastaveno správně.</p>

<pre>
<span class="highlight">exec /bin/su -s /bin/sh - kafka -c '/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties'</span>
# wait on stop (to fix restart) for two reasons
# 1) avoid jmx/rmi 'port already in use'
# 2) wait for the session to expire in ZK to avoid start-up fail
post-stop exec sleep 11
respawn
respawn limit 3 30

# http://upstart.ubuntu.com/wiki/Stanzas
# time to wait between sending TERM and KILL signals
<span class="highlight">kill timeout 300</span>

start on stopped rc RUNLEVEL=[2345]
stop on runlevel [S016]
</pre>

<p>Kde by jenom mohla být chyba? Na světě není nic dokonalé, dokonce ani software, nějaký bug někde? Ale kde? :-(</p>

<p>Zkuste se zamyslet, než budete číst dál. Představte si, že jste se dostali k poslední stránce napínavé detektivky. Víte, že už máte všechny potřebné informace a logickou úvahou máte dojít k tomu, kdo je vrah. <strong>Kdo je tedy vrah?</strong> Kdo bez jakýchkoli rozpaků brutálně zabíjí Kafku signálem <code>SIGKILL</code>?</p>


<h2>su kafka</h2>

<p>Ano, správně. Je to někdo, kdo téměř ani nebyl zmíněn, téměř neviditelný pomocník <code>/bin/su</code>. Tento užitečný prográmek je asi nejznámější soubor se <code>sticky bitem</code> a bývá mnohokrát skloňován snad v každé knize o bezpečnosti v unixových prostředích. Pojďme v <code>/bin/su</code> zkusit najít odpověď na naši otázku.</p>

<p>Anketa: Zabíjí <code>/bin/su</code> potomka signálem <code>SIGKILL</code>?</p>

<ul>
<li>Ano.</li>
<li>Ne.</li>
<li>Jiné.</li>
<li>Nevím.</li>
</ul>

<p>V tomto případě se <code>/bin/su</code> nepoužívá k typickému zvyšování práv na roota, např. kvůli instalaci nového softwaru. Používá se přesně naopak, tedy ke snížení práv spouštěného procesu, aby Kafka neběžela pod rootem.</p>


<div class="side">
<h3>Unix a procesy</h3>

<p>V operačních systémech založených na Unixu se nové procesy vytvářejí pomocí kernelových volání <code>fork()</code> a <code>exec()</code>.</p>

<p>Úkolem <code>fork()</code> je naklonovat současný proces do nového, který bude až na jedno číslo identický. Tímto číslem, je návratová hodnota funkce <code>fork()</code>. Rodičovský proces v ní obdrží identifikátor potomka a potomek nulu. Procesy se tak dozví, kým jsou, a v kódu se mohou dále rozhodnout, jaké operace budou vykonávat.</p>

<p>Volání <code>exec()</code> umožňuje nahradit současný kontext procesu nějakým jiným. Z kopie předchozího procesu se tak stane úplně jiná aplikace.</p>

<pre>
<span style='color:#898887;'>// gcc -Wall -o forkexec forkexec.c</span>
<span style='color:#898887;'>// ./forkexec</span>

<span style='color:#006e28;'>#include </span><span style='color:#006e28;'>&lt;stdio.h&gt;</span>
<span style='color:#006e28;'>#include </span><span style='color:#006e28;'>&lt;unistd.h&gt;</span>
<span style='color:#006e28;'>#include </span><span style='color:#006e28;'>&lt;sys/wait.h&gt;</span>

<span style='color:#0057ae;'>int</span> main(<span style='color:#0057ae;'>int</span> argc, <span style='color:#0057ae;'>char</span>* argv[]) {
  pid_t pid = fork();

  <b>if</b> (pid == -<span style='color:#b08000;'>1</span>) {
    fprintf(stderr, <span style='color:#bf0303;'>&quot;Fork failed</span><span style='color:#924c9d;'>\n</span><span style='color:#bf0303;'>&quot;</span>);
    <b>return</b> <span style='color:#b08000;'>1</span>;
  } <b>else</b> <b>if</b> (pid == <span style='color:#b08000;'>0</span>) {
    printf(<span style='color:#bf0303;'>&quot;Child: pid %d</span><span style='color:#924c9d;'>\n</span><span style='color:#bf0303;'>&quot;</span>, getpid());
    execl(<span style='color:#bf0303;'>&quot;/bin/echo&quot;</span>, <span style='color:#bf0303;'>&quot;/bin/echo&quot;</span>,
          <span style='color:#bf0303;'>&quot;Echo: Hello, world!&quot;</span>, NULL);
    <b>return</b> <span style='color:#b08000;'>42</span>; <span style='color:#898887;'>// Never executed</span>
  } <b>else</b> {
    printf(<span style='color:#bf0303;'>&quot;Parent: pid %d, child %d</span><span style='color:#924c9d;'>\n</span><span style='color:#bf0303;'>&quot;</span>,
           getpid(), pid);
    <span style='color:#0057ae;'>int</span> status;
    waitpid(pid, &amp;status, <span style='color:#b08000;'>0</span>);
    printf(<span style='color:#bf0303;'>&quot;Parent: child exited %d</span><span style='color:#924c9d;'>\n</span><span style='color:#bf0303;'>&quot;</span>, status);
    <b>return</b> <span style='color:#b08000;'>0</span>;
  }
}
</pre>

<pre>
./forkexec
Parent: pid 9574, child 9575
Child: pid 9575
Echo: Hello, world!
Parent: child exited 0
</pre>

<p><code>fork()</code> a <code>exec()</code> se nemusí používat pouze společně. Pokud by se zavolal pouze <code>exec()</code>, nedošlo by k naklonování procesu a spuštěná aplikace by se pouze nahradila nějakou jinou. Přesně toho chceme dosáhnout v tomto článku.</p>

<p>V případě samostatného volání <code>fork()</code> by se v potomku nenahradil současný kontext za jiný a v systému by defakto běžely dvě kopie původního procesu. Toho se využívá například ve webových serverech, kdy nginx nebo Apache vytváří skupinu tzv. worker procesů, kterým rodič přeposílá práci.</p>

<p>Procesy se v systému označují unikátním identifikátorem PID (Process ID). Platné hodnoty začínají číslem 1 a končí konstantou definovanou v <code>/proc/sys/kernel/pid_max</code>, která má typicky hodnotu 32768 popř. 4194303. Procesy se organizují do stromové struktury, prvním spuštěným je <code>init</code> s PID 1 (System V init, Upstart, SystemD a další) a má speciální povinnosti.</p>
</div>


<h2>Do hlubin /bin/su</h2>

<p>Zkusme v rychlosti analyzovat chování /bin/su. V prvním terminálu pod rootem spustíme příkaz obdobný Upstart konfiguraci. Příkaz <code>exec</code> se před <code>sleep</code> nachází úmyslně, proces JVM se při startu Kafky spouští úplně stejně (viz kód <code>kafka-server-start.sh</code>, resp. <code>kafka-run-class.sh</code>).</p>

<pre>
# Become root
su -

# Lower privileges and execute something
exec /bin/su -s /bin/sh - nobody -c 'exec /bin/sleep 60'
</pre>

<p>V druhém terminálu se podíváme na strom procesů. Oba příkazy /bin/su se zachovaly, <code>exec</code> nahradil pouze shell v potomcích. Tímto pokusem jsme zjistili, že Upstart posílá SIGTERM /bin/su a ne JVM procesu, ve kterém běží Kafka.</p>

<pre>
pstree | less

# After first su
init-+
     |-xterm---bash---su---bash

# After second su
init-+
     |-xterm---bash---su---su---sleep
</pre>

<p>Pokus číslo dva a <code>strace</code>. Jaká volání systému vykoná /bin/su po příchozím SIGTERMu?</p>

<pre>
strace /bin/su -s /bin/sh - nobody -c 'exec /bin/sleep 60' 2> strace_su.log

kill -s TERM 11827
</pre>

<p>Hledáme klíčová slova <code>kill</code>, <code>SIGTERM</code> a <code>SIGKILL</code>. Podle výstupu /bin/su nejdříve pošle potomkovi <code>SIGTERM</code>, poté počká 2 sekundy a aby mělo jistotu, zabije ho pomocí <code>SIGKILL</code>. <strong>Jenom dvě sekundy?</strong> Máme odpověď, ale proč jenom dvě sekundy?</p>

<pre>
...
--- SIGTERM {si_signo=SIGTERM, si_code=SI_USER, si_pid=10805, si_uid=0} ---
rt_sigreturn()                          = -1 EINTR (Interrupted system call)
write(2, "\nSession terminated, killing she"..., 37
Session terminated, killing shell...) = 37
<span class="highlight">kill(11828, SIGTERM)                    = 0</span>
...
rt_sigprocmask(SIG_BLOCK, [CHLD], ~[INT QUIT KILL ALRM TERM STOP RTMIN RT_1], 8) = 0
<span class="highlight">nanosleep({2, 0}, 0x7ffc0c569e40)       = 0</span>
<span class="highlight">kill(11828, SIGKILL)                    = 0</span>
write(2, " ...killed.\n", 12 ...killed.
)           = 12
rt_sigaction(SIGTERM, {SIG_DFL, [], SA_RESTORER, 0x7f4d60a62660}, NULL, 8) = 0
kill(11827, SIGTERM)                    = 0
--- SIGTERM {si_signo=SIGTERM, si_code=SI_USER, si_pid=11827, si_uid=0} ---
+++ killed by SIGTERM +++
</pre>


<h2>Zdrojové kódy /bin/su</h2>

<p>Najít zdrojové kódy /bin/su není úplně jednoduché, ale jde to zvládnout. Na serverch s Kafkou toho času běží stařičký CentOS 6.</p>

<pre>
rpm -qf /bin/su
coreutils-8.4-43.el6.x86_64
</pre>

<p>Po vyhledání balíku a konkrétní verze dorazíme na <a href="https://rpmfind.net/linux/RPM/centos/6.8/x86_64/Packages/coreutils-8.4-43.el6.x86_64.html">rpmfind.net</a>, což sice není autoritativní zdroj, ale pro naše účely postačí. Na stránce je zároveň uveden odkaz na zdrojový balík, ve kterém by se měly nacházet zdrojové kódy. Stáhneme ho a rozbalíme. Po chvíli hledání objevíme soubor <code>coreutils-pam.patch</code> a kód, který odpovídá výstupu <code>strace</code>.</p>

<pre>
+  if (caught) {
+    fprintf(stderr, "\nSession terminated, killing shell...");
<span class="highlight">+    kill (child, SIGTERM);</span>
+  }
+  /* Not checking retval on this because we need to call close session */
+  pam_setcred(pamh, PAM_DELETE_CRED | PAM_SILENT);
+  retval = pam_close_session(pamh, 0);
+  PAM_BAIL_P_VOID;
+  retval = pam_end(pamh, PAM_SUCCESS);
+  PAM_BAIL_P_VOID;
+  if (caught) {
<span class="highlight">+    sleep(2);</span>
<span class="highlight">+    kill(child, SIGKILL);</span>
+    fprintf(stderr, " ...killed.\n");
+    exit(-1);
+  }
</pre>

<p>Pravděpodobně se jedná o backport nějaké důležité funkcionality z upstream verze do starého systému. Autorem patche je pravděpodobně někdo z RedHatu.</p>

<p>Výše uvedený <a href="https://rpmfind.net/linux/RPM/centos/6.8/x86_64/Packages/coreutils-8.4-43.el6.x86_64.html">rpmfind.net</a> uvádí jako zdroj <a href="http://www.gnu.org/software/coreutils/coreutils.html">gnu.org/coreutils</a>, nicméně projekt nebo jeho část se někdy v minulosti přesunul na <a href="https://git.kernel.org/pub/scm/utils/util-linux/util-linux.git/about/">kernel.org/util-linux</a>. S <code>git annotate</code> se dá v dávné historii dohledat <a href="https://github.com/karelzak/util-linux/commit/8171142ab66e94a409224547b33381259a0c3f72">původní zdrojový commit</a>, bohužel jeho popis neobsahuje moc relevantních informací. Pro zájemce uvádím i odkaz na <a href="https://github.com/karelzak/util-linux/blob/master/login-utils/su-common.c">zdrojový kód nejnovější verze /bin/su</a>.</p>

<pre>
commit 8171142ab66e94a409224547b33381259a0c3f72
Author: Ludwig Nussel &lt;ludwig.nussel@suse.de&gt;
Date:   Tue Aug 17 13:21:44 2010 +0200

    pam support for su
</pre>

<p>Tento bug nebo možná pouze feature jsme samozřejmě poctivě <a href="https://github.com/karelzak/util-linux/issues/443">nahlásili</a> a dokonce i <a href="https://github.com/karelzak/util-linux/pull/444">opravili</a>. Po diskuzi s vývojáři (zdravíme Karla Žáka) to nevypadá, že by změna v kódu byla někdy aplikována. Ačkoli změna proti současnému stavu dává smysl, protože by se /bin/su chovalo více konzistentně, nemáme motivaci se ji pokusit protlačit.</p>


<h2>Není su jako su</h2>

<p>Nyní v článku trochu odbočíme a budeme se věnovat jednomu překvapivému zjištění. Pokud byste si zkusili výše uvedené příklady na Debianu, nebudou vám vůbec fungovat. Výstup <code>strace</code> od pohledu vypadá úplně jinak a hlavně neobsahuje žádný <code>SIGKILL</code>.</p>

<pre>
...
--- SIGTERM {si_signo=SIGTERM, si_code=SI_USER, si_pid=30289, si_uid=0} ---
rt_sigreturn()                          = -1 EINTR (Interrupted system call)
write(2, "\n", 1
)                       = 1
open("/usr/share/locale/locale.alias", O_RDONLY|O_CLOEXEC) = 7
fstat(7, {st_mode=S_IFREG|0644, st_size=2492, ...}) = 0
mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f5f5e7dc000
read(7, "# Locale name alias data base.\n#"..., 4096) = 2492
read(7, "", 4096)                       = 0
close(7)                                = 0
munmap(0x7f5f5e7dc000, 4096)            = 0
open("/usr/share/locale/en_US/LC_MESSAGES/shadow.mo", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/usr/share/locale/en/LC_MESSAGES/shadow.mo", O_RDONLY) = -1 ENOENT (No such file or directory)
write(2, "Session terminated, terminating "..., 40Session terminated, terminating shell...) = 40
<span class="highlight">kill(4294936960, SIGTERM)               = 0</span>
rt_sigaction(SIGALRM, {0x55b0bac7ef90, [ALRM], SA_RESTORER|SA_RESTART, 0x7f5f5de3d0e0}, {SIG_DFL, [], SA_RESTORER, 0x7f5f5de3d0e0}, 8) = 0
rt_sigaction(SIGCHLD, {0x55b0bac7ef30, [CHLD], SA_RESTORER|SA_RESTART, 0x7f5f5de3d0e0}, {SIG_DFL, [], 0}, 8) = 0
alarm(2)                                = 0
rt_sigprocmask(SIG_BLOCK, [ALRM], NULL, 8) = 0
wait4(30336, [{WIFSIGNALED(s) && WTERMSIG(s) == SIGTERM}], WNOHANG, NULL) = 30336
rt_sigprocmask(SIG_UNBLOCK, [ALRM], NULL, 8) = 0
write(2, " ...terminated.\n", 16 ...terminated.
)       = 16
...
exit_group(143)                         = ?
+++ exited with 143 +++
</pre>

<p>Může za to stáří systému? CentOS 6 je výrazně starší než současný a taktéž hodně starý Debian Stable/Jessie. Tím to nebude, vždyť jsme si ověřili, že se <code>SIGKILL</code> nachází i v nejnovější verzi /bin/su a dokonce jsme udělali i opravu, která tuto funkcionalitu odstraňuje. Nebo že by se jednalo o nějaký distribuční patch, který by opravoval právě tuto chybu? Možné je všechno.</p>

<p>Pojďme si opět projít zdrojové kódy.</p>

<pre>
dpkg -S /bin/su
login: /bin/su
</pre>

<p>Balík má jiné jméno - <a href="https://packages.debian.org/jessie/login">login</a> a zdrojový balík je také jiný - <a href="https://packages.debian.org/jessie/login">shadow</a>. Chyba v matrixu? Proč taková změna? Tentokrát nebudu napínat, <strong>/bin/su na Debianu je úplně jiná aplikace než /bin/su na CentOSu!</strong> Cože?! :-O</p>

<p>Už je všechno v pořádku? Vstřebali jste to? Dobře, tak zavřete pusu. Debianí projekt sídlí na <a href="http://pkg-shadow.alioth.debian.org/">pkg-shadow.alioth.debian.org</a> a moc toho o sobě neříká. Vývojáři Debianu se možná rozhodli pro vlastní implementaci tohoto důležitého subsystému, aby ho měli plně pod kontrolou. Ale to jenom hádáme.</p>

<p>Je moc hezké vidět principy objektového programování - více implementací interface s do detailů splněnými kontrakty - i na úrovni nejzákladnějších unixových utilit. Programátora zkrátka něco takového zahřeje na srdci ;-)</p>

<p>Jak jsme na něco takového přišli? Poté, co David objevil příčinu problému v /bin/su, zkusil jsem při psaní retrospektivy najít poslaný kousek kódu v upstream zdrojových kódech. A protože mám u sebe na počítači Debian, hledal jsem jiným směrem a došel do jiného cíle. Jedním slovem náhoda.</p>


<h2>Jak z toho ven?</h2>

<p>Vraťme se zpět k našemu problému a především k možnostem jeho řešení. Je jasné, že patchovat a provozovat vlastní verzi /bin/su není něco, do čeho by se nám zrovna chtělo. Nicméně použití tohoto nástroje byla čistě naše volba a nikdo nás do něho nenutí.</p>


<h3>su, sudo</h3>

<p>Co je vlastně hlavním nedostatkem /bin/su (v této situaci, ne obecně)? Především to, že s ním vzniká hned několik podprocesů a Upstart operuje s PID předka místo potomka. Na začátku se spouští /bin/su, /bin/su spouští login shell a shell spouští příkaz uvedený v parametrech. A při ukončování procesu Upstart posílá SIGTERM /bin/su a ne Kafce. Tento řetěz několika fork-exec operací je mnohem složitější, než by mohl být. Místo něj by úplně stačilo přepnout efektivní UID z roota na neprivilegovaného uživatele a následně zavolat exec, který přepíše současný obsah procesu spouštěnou aplikací. Jednoho by mohl napadnout alternativní příkaz sudo, ten se však chová obdobně a výsledná situace by byla naprosto stejná. Ani jeden z nich se pro tento use case naprosto nehodí, nebyly na něj vůbec navrženy, a je tedy zbytečné je dále uvažovat.</p>


<h3>Upstart</h3>

<p>Úplně nejjednodušší by bylo pomocí direktivy <a href="http://upstart.ubuntu.com/cookbook/#setuid"><code>setuid &lt;username&gt;</code></a> požádat Upstart, aby špinavou práci s přepínáním uživatele odvedl sám a nás tím nezatěžoval. Bohužel tato možnost je dostupná až od verze 1.4, kdežto v CentOSu 6 je dostupná pouze výrazně starší 0.6.5. Update kvůli podobnému problému by byl pouze v říši fantazie.</p>

<p>Dokumentace Upstartu popisuje, jak správně <a href="http://upstart.ubuntu.com/cookbook/#run-a-job-as-a-different-user">spustit job pod jiným uživatelem</a>. Jako jednu z doporučovaných možností uvádí utilitu <code>start-stop-daemon</code> dostupnou v Debianu a Ubuntu, která "jednoduše spustí (exec) daný příkaz poté, co změní UID/GID", což je přesně to, k čemu jsme došli sami.  U su a sudo uvádí další dosud neuvažovanou nevýhodu a to, že skrze PAM zapisují záznamy o přihlašování uživatele do <code>wtmp</code>, což je při obyčejném spouštění aplikace spíše nežádoucí chování. Téměř na konci této sekce dokumentace je mezi příklady se su a sudo jakoby mimochodem zmíněna direktiva <code>expect</code>.</p>

<p><a href="http://upstart.ubuntu.com/cookbook/#expect">Popis <code>expect</code></a> je uveden rámečkem s varováním "extrémně důležitá direktiva: čtěte tuto sekci obezřetně". Následuje několik odstavců o tom, jak se chovají různé typy aplikací, některé neforkují vůbec, jiné forkují jednou a ještě jiné dvakrát a co z toho plyne pro Upstart, který musí být schopen referencovat a spravovat správný process. Kdybychom si tuto část přečetli už kdysi, ušetřili bychom si všechny budoucí "zážitky" a problémy. S původní /bin/su variantou v konfiguračním souboru by bylo správné přidat dodatečný řádek <code>expect fork</code>, který by Upstartu oznámil, že má čekat právě jeden fork spouštěné aplikace. Tudíž by nespravoval /bin/su, ale Javu/Kafku. Tuto možnost jsme už ovšem nezkoušeli, protože jsme se rozhodli jít původně naznačenou cestou.</p>


<h3>Přepnutí uživatele</h3>

<p>Vraťme se k Debianímu <code>start-stop-daemon</code>, který bohužel na CentOSu není dostupný, a jeho alternativám. Je zajímavé zjistit, že nejspíš neexistuje žádný univerzálně dostupný nástroj, a že každá distribuce poskytuje ve výchozí instalaci něco jiného a nekompatibilního. Nástroje mají jiná jména, jiné parametry a jiné možnosti. Konfigurační soubor Upstartu by nebyl přenositelný mezi Linuxovými distribucemi (pokud pomineme fakt dostupnosti samotného Upstartu v distribucích). Samozřejmě všechno se dá nějakým způsobem doinstalovat, třeba i velice jednoduše, ale kdo by se o to chtěl starat. Tata situaci se může porovnat například se su a sudo - dá se na ně spolehnout, že budou dostupné tak nějak vždycky a všude.</p>

<ul>
    <li><code>start-stop-daemon</code> - pouze Debian a deriváty</li>
    <li><code>daemon</code> utilita z <code>/etc/init.d/functions</code> - různě po webu doporučovaná CentOS alternativa ke <code>start-stop-daemon</code></li>
    <li>další <a href="https://github.com/daleobrien/start-stop-daemon"><code>start-stop-daemon</code></a> - přepis Debianího toolu z Perlu do C</li>
    <li><a href="https://git.kernel.org/pub/scm/utils/util-linux/util-linux.git/tree/sys-utils/setpriv.c"><code>setpriv</code></a> - součást util-linux, <a href="https://github.com/karelzak/util-linux/issues/443#issuecomment-304610565">zmíněný v diskuzi u reportu o chybě v /bin/su</a>, ale není ani v CentOSu ani v Debianu</li>
    <li><a href="https://github.com/tianon/gosu">gosu</a> - používaný v kontejnerech a Docker světě</li>
    <li>Python: <code><a href="https://docs.python.org/3/library/os.html">import os</a> ; os.setuid(...) ; os.exec*(...)</code>. Při použití pozor na <a href="https://www.dwheeler.com/secure-programs/Secure-Programs-HOWTO/environment-variables.html">reset prostředí</a>!</li>
    <li>A spousty dalších možností...</li>
</ul>

<p>Shodou okolností máme vlastní nástroj <code>ff-runner</code>, který byl před lety vytvořen ke standartizaci spouštění našich serverových aplikací. Jednou z jeho vlastností je mimo jiné přepnutí na požadovaného uživatele a exec procesu (bez forku), už ho máme na serverech, takže volba byla celkem jasná.</p>

</div>
</body>
</html>